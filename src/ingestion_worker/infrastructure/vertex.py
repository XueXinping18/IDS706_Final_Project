"""
èŒè´£ï¼š
- è°ƒç”¨ Gemini APIï¼ˆæ”¯æŒ Function Callingï¼‰
- å¤„ç† function_call/function_response å¾ªç¯

ä¾èµ–ï¼šgoogle-cloud-aiplatform
"""
import asyncio
import json
import re
import os
from typing import Callable, Awaitable, Any, Optional
from datetime import timedelta

from vertexai.generative_models import (
    GenerativeModel,
    Part,
    Content,
    Tool,
    GenerationConfig,
    FunctionDeclaration,
    SafetySetting,
    HarmCategory,
    HarmBlockThreshold
)
from vertexai.preview.caching import CachedContent
from google.api_core import exceptions as gcp_exceptions

from ingestion_worker.config import Config
from ingestion_worker.utils.logging import get_logger


class VertexError(Exception):
    """Vertex AI è°ƒç”¨é”™è¯¯"""
    pass


class VertexClient:
    """Gemini API å®¢æˆ·ç«¯ï¼ˆçº¯æŠ€æœ¯å°è£…ï¼‰"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ– Vertex AI å®¢æˆ·ç«¯

        Args:
            config: ç³»ç»Ÿé…ç½®
        """
        self.config = config
        self.logger = get_logger(__name__)

        try:
            # åˆå§‹åŒ– Vertex AI
            import vertexai
            vertexai.init(
                project=config.gcp_project,
                location=config.gcp_region
            )
            self.logger.info(
                f"âœ“ Vertex AI åˆå§‹åŒ–æˆåŠŸ "
                f"(project: {config.gcp_project}, region: {config.gcp_region})"
            )
        except Exception as e:
            self.logger.error(f"âœ— Vertex AI åˆå§‹åŒ–å¤±è´¥: {e}")
            raise VertexError(f"Failed to initialize Vertex AI: {e}") from e

    async def create_cached_content(
        self,
        video_uri: Optional[str],
        text_content: str,
        system_instruction: Optional[str] = None,  # [ä¿æŒ] æ¥æ”¶ç³»ç»ŸæŒ‡ä»¤
        tools: Optional[list[Tool]] = None,
        ttl_seconds: Optional[int] = None
    ) -> CachedContent:
        """
        åˆ›å»ºç¼“å­˜å†…å®¹ï¼ˆåŒ…å«è§†é¢‘ã€æ–‡æœ¬å’Œtoolsï¼‰
        """
        if ttl_seconds is None:
            ttl_seconds = self.config.gemini_cache_ttl_seconds

        try:
            contents = []

            # æ·»åŠ è§†é¢‘ï¼ˆå¦‚æœæœ‰ï¼‰
            if video_uri:
                self.logger.info(f"æ·»åŠ è§†é¢‘åˆ°ç¼“å­˜: {video_uri}")
                contents.append(
                    Part.from_uri(video_uri, mime_type="video/mp4")
                )

            # æ·»åŠ æ–‡æœ¬å†…å®¹
            contents.append(Part.from_text(text_content))

            # åœ¨ asyncio ä¸­è¿è¡ŒåŒæ­¥ä»£ç 
            cached_content = await asyncio.to_thread(
                CachedContent.create,
                model_name=self.config.gemini_model,
                contents=[Content(role="user", parts=contents)],
                tools=tools,  # Include tools in cache
                system_instruction=system_instruction,  # [ä¿æŒ] æ³¨å…¥ç³»ç»ŸæŒ‡ä»¤åˆ° Cache
                ttl=timedelta(seconds=ttl_seconds)
            )

            self.logger.info(
                f"âœ“ ç¼“å­˜å·²åˆ›å»º: {cached_content.name}, TTL={ttl_seconds}s"
            )

            return cached_content

        except gcp_exceptions.InvalidArgument as e:
            self.logger.error(f"å‚æ•°æ— æ•ˆ: {e}")
            raise VertexError(f"Invalid argument: {e}") from e
        except gcp_exceptions.ResourceExhausted as e:
            self.logger.error(f"é…é¢è€—å°½: {e}")
            raise VertexError(f"Quota exceeded: {e}") from e
        except Exception as e:
            self.logger.error(f"åˆ›å»ºç¼“å­˜å¤±è´¥: {e}")
            raise VertexError(f"Failed to create cached content: {e}") from e

    async def call_with_tools(
        self,
        cached_content: Optional[CachedContent],
        prompt: str,
        tools: list[Tool],
        tool_handler: Callable[[str, dict], Awaitable[dict]],
        system_instruction: Optional[str] = None,  # [ä¿æŒ] ç”¨äºæ— ç¼“å­˜æ¨¡å¼
        generation_config: Optional[dict] = None,
        trace_context: Optional[dict] = None  # [æ–°å¢] è¿½è¸ªä¸Šä¸‹æ–‡
    ) -> dict:
        """
        è°ƒç”¨ Gemini å¹¶å¤„ç† Function Calling å¾ªç¯

        Args:
            trace_context: è¿½è¸ªä¸Šä¸‹æ–‡ {segment_index, segment_text, annotator_kind, video_uid}
        """
        try:
            config = GenerationConfig(
                temperature=0.0,
                max_output_tokens=8192,
                **(generation_config or {})
            )

            # åˆ›å»ºæ¨¡å‹
            if cached_content:
                # ç¼“å­˜æ¨¡å¼ï¼šsystem_instruction å’Œ tools å·²ç»åœ¨ cached_content ä¸­
                model = GenerativeModel.from_cached_content(cached_content)
            else:
                # æ— ç¼“å­˜æ¨¡å¼ï¼šæ˜¾å¼ä¼ å…¥ system_instruction å’Œ tools
                # Configure safety settings to be permissive
                safety_settings = [
                    SafetySetting(
                        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                        threshold=HarmBlockThreshold.BLOCK_NONE,
                    ),
                    SafetySetting(
                        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                        threshold=HarmBlockThreshold.BLOCK_NONE,
                    ),
                    SafetySetting(
                        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                        threshold=HarmBlockThreshold.BLOCK_NONE,
                    ),
                    SafetySetting(
                        category=HarmCategory.HARM_CATEGORY_HARASSMENT,
                        threshold=HarmBlockThreshold.BLOCK_NONE,
                    ),
                ]

                # å®ä¾‹åŒ–æ¨¡å‹
                model = GenerativeModel(
                    model_name=self.config.gemini_model,
                    system_instruction=system_instruction,
                    tools=tools,
                    safety_settings=safety_settings
                )

            chat = model.start_chat(response_validation=False)

            # æå–è¿½è¸ªä¿¡æ¯
            ctx = trace_context or {}
            video_uid = ctx.get("video_uid", "N/A")
            seg_idx = ctx.get("segment_index", "N/A")
            seg_text = ctx.get("segment_text", "N/A")[:200]  # æˆªå–å‰200å­—ç¬¦
            annotator = ctx.get("annotator_kind", "N/A")
            trace_id = f"[{video_uid}|Seg#{seg_idx}|{annotator}]"

            # ========== ä½ç½® 1: ç¬¬ä¸€æ¬¡è¯·æ±‚å‰ ==========
            self.logger.info("=" * 80)
            self.logger.info(f"ğŸ“¤ {trace_id} Gemini è¯·æ±‚")
            self.logger.info(f"   Segment: \"{seg_text}\"")
            self.logger.info(f"   Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦ | ç¼“å­˜: {cached_content.name if cached_content else 'No cache'}")
            self.logger.info("=" * 80)
            self.logger.debug(f"{trace_id} å®Œæ•´ Prompt:\n{prompt}")

            # ç¬¬ä¸€æ¬¡è°ƒç”¨
            # å¦‚æœä½¿ç”¨ cached_contentï¼Œtools å·²åœ¨ cache ä¸­ï¼Œä¸èƒ½å†ä¼ 
            # å¦‚æœä¸ç”¨ cacheï¼Œä¼  tools (æ— ç¼“å­˜æ¨¡å¼ä¸‹ tools å·²åœ¨ model åˆå§‹åŒ–æ—¶ä¼ å…¥)
            response = await asyncio.to_thread(
                chat.send_message,
                prompt,
                generation_config=config
            )

            # ========== ä½ç½® 1: ç¬¬ä¸€æ¬¡å“åº”å ==========
            self.logger.info(f"ğŸ“¥ {trace_id} Gemini ç¬¬1æ¬¡å“åº”")
            self.logger.info(f"   Segment: \"{seg_text}\"")
            if response.candidates:
                self.logger.info(f"   Finish reason: {response.candidates[0].finish_reason}")
            self.logger.debug(f"{trace_id} å®Œæ•´å“åº”å¯¹è±¡: {response}")

            # å¤„ç† function calling å¾ªç¯
            max_iterations = 10  # é˜²æ­¢æ— é™å¾ªç¯
            iteration = 0

            # è®°å½• fine_id â†’ lemma çš„æ˜ å°„ï¼ˆç”¨äºæœ€ç»ˆå†³ç­–æ—¶æ˜¾ç¤ºï¼‰
            fine_id_to_lemma = {}

            while iteration < max_iterations:
                iteration += 1

                # æ£€æŸ¥æ˜¯å¦æœ‰ function call
                function_calls = self._extract_function_calls(response)

                # ========== ä½ç½® 2: æå– function_calls å ==========
                if not function_calls:
                    # æ²¡æœ‰ function callï¼Œè¿”å›æœ€ç»ˆç»“æœ
                    final_result = self._parse_response(response, ctx)
                    self.logger.info("=" * 80)
                    self.logger.info(f"âœ… {trace_id} LLM æœ€ç»ˆå†³ç­–")
                    self.logger.info(f"   Segment: \"{seg_text}\"")
                    self.logger.info(f"   æ ‡æ³¨æ•°é‡: {len(final_result.get('annotations', []))}")
                    self.logger.info("=" * 80)

                    # è®°å½•æœ€ç»ˆé€‰æ‹©çš„annotationsï¼ˆæ˜¾ç¤º lemma è€Œé spanï¼‰
                    for ann_idx, ann in enumerate(final_result.get('annotations', [])):
                        fine_id = ann.get('fine_id')
                        lemma = fine_id_to_lemma.get(fine_id, "æœªçŸ¥")
                        self.logger.info(
                            f"   [{ann_idx+1}] \"{lemma}\" â†’ fine_id={fine_id} | "
                            f"rationale: {ann.get('rationale', '')[:120]}"
                        )
                    return final_result

                self.logger.info("=" * 80)
                self.logger.info(f"ğŸ” {trace_id} LLM è°ƒç”¨å·¥å…·ï¼ˆç¬¬{iteration}è½®ï¼‰")
                self.logger.info(f"   Segment: \"{seg_text}\"")
                self.logger.info(f"   æŸ¥è¯¢æ•°é‡: {len(function_calls)}")
                for idx, fc in enumerate(function_calls):
                    self.logger.info(f"   [{idx+1}] {fc.name}({dict(fc.args)})")
                self.logger.info("=" * 80)

                # æ‰§è¡Œæ‰€æœ‰ function calls
                function_responses = []
                for idx, fc in enumerate(function_calls):
                    name = fc.name
                    args = dict(fc.args)

                    self.logger.debug(f"è°ƒç”¨å·¥å…·: {name}({args})")

                    try:
                        # ========== ä½ç½® 3: æ‰§è¡Œå·¥å…·è°ƒç”¨ ==========
                        result = await tool_handler(name, args)

                        # æå– lemma å¹¶å»ºç«‹ fine_id â†’ lemma æ˜ å°„
                        if isinstance(result, dict) and "lemma" in result and "candidates" in result:
                            lemma = result["lemma"]
                            candidates = result["candidates"]
                            for cand in candidates:
                                if isinstance(cand, dict) and "fine_id" in cand:
                                    fine_id_to_lemma[cand["fine_id"]] = lemma

                        self.logger.info(f"  âœ“ å·¥å…·è°ƒç”¨æˆåŠŸ [{idx+1}]: {name}")
                        function_responses.append(
                            Part.from_function_response(
                                name=name,
                                response={"result": result}
                            )
                        )
                    except Exception as e:
                        self.logger.error(f"  âœ— å·¥å…·è°ƒç”¨å¤±è´¥ [{idx+1}]: {name}, é”™è¯¯: {e}")
                        function_responses.append(
                            Part.from_function_response(
                                name=name,
                                response={"error": str(e)}
                            )
                        )

                # ========== ä½ç½® 4: å‘é€ function_responses å‰ ==========
                self.logger.info(f"ğŸ“¤ {trace_id} å‘é€å·¥å…·ç»“æœç»™ LLMï¼ˆç¬¬{iteration}è½®ï¼‰")
                self.logger.info(f"   Segment: \"{seg_text}\"")

                # å‘é€ function responses
                response = await asyncio.to_thread(
                    chat.send_message,
                    function_responses,
                    generation_config=config
                )

                # ========== ä½ç½® 4: æ”¶åˆ° Gemini å“åº”å ==========
                self.logger.info(f"ğŸ“¥ {trace_id} LLM ç¬¬{iteration+1}æ¬¡å“åº”ï¼ˆå¤„ç†å·¥å…·ç»“æœåï¼‰")
                self.logger.info(f"   Segment: \"{seg_text}\"")
                if response.candidates:
                    self.logger.info(f"   Finish reason: {response.candidates[0].finish_reason}")

            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            self.logger.warning(
                f"Function calling è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})"
            )
            return self._parse_response(response, ctx)

        except gcp_exceptions.DeadlineExceeded as e:
            self.logger.error(f"è¯·æ±‚è¶…æ—¶: {e}")
            raise VertexError(f"Request timeout: {e}") from e
        except gcp_exceptions.ResourceExhausted as e:
            self.logger.error(f"é…é¢è€—å°½: {e}")
            raise VertexError(f"Quota exceeded: {e}") from e
        except Exception as e:
            self.logger.error(f"Gemini è°ƒç”¨å¤±è´¥: {e}")
            raise VertexError(f"Failed to call Gemini: {e}") from e

    def _extract_function_calls(self, response) -> list:
        """
        ä»å“åº”ä¸­æå– function calls

        Args:
            response: GenerateContentResponse

        Returns:
            FunctionCall åˆ—è¡¨
        """
        function_calls = []

        for candidate in response.candidates:
            for part in candidate.content.parts:
                if hasattr(part, 'function_call') and part.function_call:
                    function_calls.append(part.function_call)

        return function_calls

    def _parse_response(self, response, trace_context: dict = None) -> dict:
        """
        è§£æ Gemini å“åº”ä¸º dict

        Args:
            response: GenerateContentResponse

        Returns:
            è§£æåçš„ JSON å¯¹è±¡

        Raises:
            VertexError: è§£æå¤±è´¥
        """
        try:
            # è·å–ç¬¬ä¸€ä¸ªå€™é€‰çš„æ–‡æœ¬
            if not response.candidates:
                self.logger.error("Gemini å“åº”ä¸­æ²¡æœ‰ candidates")
                raise VertexError("No candidates in response")

            candidate = response.candidates[0]

            # è®°å½•å€™é€‰çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºè¯Šæ–­ï¼‰
            self.logger.debug(f"Candidate finish_reason: {candidate.finish_reason}")
            self.logger.debug(f"Candidate content: {candidate.content}")

            if not candidate.content.parts:
                # âš ï¸ Gemini è¿”å›ç©ºå†…å®¹ - è§†ä¸ºè¯¥ segment æ— éœ€æ ‡æ³¨
                ctx = trace_context or {}
                self.logger.warning("=" * 80)
                self.logger.warning("âš ï¸ GEMINI è¿”å›ç©ºå“åº”ï¼ˆè§†ä¸ºæ— éœ€æ ‡æ³¨ï¼‰")
                self.logger.warning(f"ğŸ“ Segment #{ctx.get('segment_index', 'N/A')}: \"{ctx.get('segment_text', 'N/A')}\"")
                self.logger.warning(f"ğŸ“ Annotator: {ctx.get('annotator_kind', 'N/A')}")
                self.logger.warning(f"ğŸ“ finish_reason: {candidate.finish_reason}")
                self.logger.warning("=" * 80)

                # è¿”å›ç©ºçš„ annotations æ•°ç»„ï¼Œè€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯
                return {"annotations": []}

            text = candidate.content.parts[0].text
            
            # Clean up markdown code blocks if present
            if "```" in text:
                # Extract content between first ```json (or just ```) and last ```
                match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
                if match:
                    text = match.group(1)
                else:
                    # Fallback: just strip backticks
                    text = text.replace("```json", "").replace("```", "").strip()

            # Ensure we have a valid JSON object structure
            text = text.strip()
            
            # Debug: Write raw response to file
            try:
                with open("gemini_debug_response.txt", "w", encoding="utf-8") as f:
                    f.write(text)
                self.logger.info(f"Raw response saved to: {os.path.abspath('gemini_debug_response.txt')}")
            except Exception as e:
                self.logger.error(f"Failed to save debug response: {e}")

            if not (text.startswith("{") and text.endswith("}")):
                 # Try to find the first { and last }
                 start = text.find("{")
                 end = text.rfind("}")
                 if start != -1 and end != -1:
                     text = text[start:end+1]

            # å°è¯•è§£æ JSON
            data = json.loads(text)

            return data

        except json.JSONDecodeError as e:
            self.logger.error(f"JSON è§£æå¤±è´¥: {e}")
            if 'text' in locals():
                self.logger.error(f"å“åº”æ–‡æœ¬ï¼ˆå‰500å­—ç¬¦ï¼‰: {text[:500]}")
            raise VertexError(f"Invalid JSON response: {e}") from e
        except Exception as e:
            self.logger.error(f"å“åº”è§£æå¤±è´¥: {e}")
            raise VertexError(f"Failed to parse response: {e}") from e